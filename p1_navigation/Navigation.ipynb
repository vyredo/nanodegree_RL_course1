{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['PATH'] = f\"{os.environ['PATH']}:/home/student/.local/bin\"\n",
    "# os.environ['PATH'] = f\"{os.environ['PATH']}:/opt/conda/lib/python3.10/site-packages\"\n",
    "\n",
    "os.environ['PATH'] = f\"{os.environ['PATH']}:/home/vidy/.local/bin\"\n",
    "os.environ['PATH'] = f\"{os.environ['PATH']}:/home/vidy/mambaforge/envs/py310/lib/python3.10/site-packages\"\n",
    "\n",
    "\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy==1.26.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip freeze | grep numpy\n",
    "!pip install . > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found path: /home/vidy/RL_banana/Value-based-methods/p1_navigation/Banana_Linux/Banana.x86_64\n",
      "Mono path[0] = '/home/vidy/RL_banana/Value-based-methods/p1_navigation/Banana_Linux/Banana_Data/Managed'\n",
      "Mono config path = '/home/vidy/RL_banana/Value-based-methods/p1_navigation/Banana_Linux/Banana_Data/MonoBleedingEdge/etc'\n",
      "Preloaded 'ScreenSelector.so'\n",
      "Preloaded 'libgrpc_csharp_ext.x64.so'\n",
      "Unable to preload the following plugins:\n",
      "\tScreenSelector.so\n",
      "\tlibgrpc_csharp_ext.x86.so\n",
      "Logging to /home/vidy/.config/unity3d/Unity Technologies/Unity Environment/Player.log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BananaBrain': <unityagents.brain.BrainInfo at 0x7f6251779de0>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# Path to the Unity environment binary\n",
    "env_path = \"Banana_Linux/Banana.x86_64\"\n",
    "\n",
    "# Initialize the UnityEnvironment\n",
    "env = UnityEnvironment(file_name=env_path, no_graphics=True)\n",
    "\n",
    "\n",
    "# Reset the environment\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [0.         1.         0.         0.         0.16895212 0.\n",
      " 1.         0.         0.         0.20073597 1.         0.\n",
      " 0.         0.         0.12865657 0.         1.         0.\n",
      " 0.         0.14938059 1.         0.         0.         0.\n",
      " 0.58185619 0.         1.         0.         0.         0.16089135\n",
      " 0.         1.         0.         0.         0.31775284 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "brain = env.brains[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Q Network \n",
    "\n",
    "- This Model use Dueling Deep Q Network or (Dueling DQN)\n",
    "- for memory, I try with ReplayMemory but I change to Prioritized Replay Memory to reduce training time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from dqn_dueling import DQNDueling as DQN\n",
    "import torch\n",
    "from experience_replay import ReplayMemory\n",
    "from prioritized_replay import PrioritizedReplayMemory\n",
    "import itertools\n",
    "import random\n",
    "from torch import nn\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "# 'Agg': used to generate plots as images and save them to a file instead of\n",
    "# rendering to screen\n",
    "matplotlib.use('Agg')\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "\n",
    "class DqnTrainer:\n",
    "    def __init__(self, id, brain_name, RUNS_DIR, DATE_FORMAT ):\n",
    "        self.env_id = id\n",
    "        self.DATE_FORMAT = DATE_FORMAT\n",
    "        self.brain_name = brain_name\n",
    "\n",
    "        # Loss function\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = None\n",
    "        \n",
    "        # Path to Run info\n",
    "        self.LOG_FILE = os.path.join(\n",
    "            RUNS_DIR, f'{self.env_id}.log')\n",
    "        self.MODEL_FILE = os.path.join(\n",
    "            RUNS_DIR, f'{self.env_id}.pt')\n",
    "        self.GRAPH_FILE = os.path.join(\n",
    "            RUNS_DIR, f'{self.env_id}.png')\n",
    "\n",
    "    def next_step(self, env, action):\n",
    "        env_info = env.step(action.item())[self.brain_name]\n",
    "        new_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        terminated = env_info.local_done[0]\n",
    "        return new_state, reward, terminated\n",
    "\n",
    "    def train(self, env, env_info, num_states, num_actions, device, continue_from_checkpoint=False):\n",
    "        brain = env.brains[self.brain_name]\n",
    "        self.device = device\n",
    "        self.action_size = brain.vector_action_space_size\n",
    "\n",
    "        print('fc1_nodes = ',self.fc1_nodes)\n",
    "        print('mini_batch_size = ',  self.mini_batch_size )\n",
    "        policy_dqn = DQN(num_states, num_actions, self.fc1_nodes).to(device)\n",
    "        target_dqn = DQN(num_states, num_actions, self.fc1_nodes).to(device)\n",
    "        epsilon_history = []\n",
    "        rewards_per_episode = []\n",
    "        start_episode = 0 \n",
    "        if continue_from_checkpoint:\n",
    "            checkpoint = torch.load(self.MODEL_FILE)\n",
    "            start_episode = checkpoint.get('step_count', 0) \n",
    "\n",
    "            with open(f\"{self.env_id}_replay_memory.pkl\", 'rb') as f:\n",
    "                PERmemory = pickle.load(f)\n",
    "            PERmemory.alpha = 0.6 \n",
    "            print(f\"Loaded replay memory with {len(PERmemory)} transitions.\")\n",
    "\n",
    "            # Restore models\n",
    "            policy_dqn.load_state_dict(checkpoint['policy_model_state_dict'])\n",
    "            target_dqn.load_state_dict(checkpoint['target_model_state_dict'])\n",
    "\n",
    "            # Restore optimizer\n",
    "            self.optimizer = torch.optim.Adam(\n",
    "                policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "            # Restore other variables\n",
    "            epsilon = checkpoint.get('epsilon', self.epsilon_init)\n",
    "            step_count = checkpoint.get('step_count', 0)\n",
    "            best_rewards = checkpoint.get('best_rewards', -9999999)\n",
    "            print(f\"Resumed from step count: {step_count}, best rewards: {best_rewards}\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"Initial trainings\")\n",
    "\n",
    "            target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "            # Policy Network optimizer, Adam Optimizer\n",
    "            self.optimizer = torch.optim.Adam(\n",
    "                policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "\n",
    "            PERmemory = PrioritizedReplayMemory(maxlen=self.replay_memory_size, alpha=0.6)\n",
    "\n",
    "            # memory = ReplayMemory(self.replay_memory_size)\n",
    "\n",
    "            epsilon = self.epsilon_init\n",
    "\n",
    "            step_count = 0\n",
    "            best_rewards = -9999999\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        last_graph_update_time = start_time\n",
    "\n",
    "        log_message = f\"{start_time.strftime(self.DATE_FORMAT)}: Training starting...\"\n",
    "        print(log_message)\n",
    "        with open(self.LOG_FILE, 'w') as file:\n",
    "            file.write(log_message + '\\n')\n",
    "            \n",
    "\n",
    "\n",
    "        for episode in itertools.count(start=start_episode):\n",
    "            env_info = env.reset(train_mode=True)[self.brain_name]\n",
    "            state = env_info.vector_observations[0]\n",
    "            state = torch.tensor(state, dtype=torch.float, device=device)\n",
    "            terminated = False\n",
    "            transition = None\n",
    "            episode_reward = 0.0\n",
    "            td_error = 1.0\n",
    "            while not terminated:\n",
    "                if random.random() < epsilon:\n",
    "                    action = np.random.randint(self.action_size)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        action = policy_dqn(\n",
    "                            state.unsqueeze(dim=0)).squeeze().argmax()\n",
    "                \n",
    "                action = torch.tensor(action, dtype=torch.int64, device=device)\n",
    "\n",
    "                new_state, reward, terminated = self.next_step(env, action)\n",
    "                episode_reward += reward\n",
    "                new_state = torch.tensor(\n",
    "                    new_state, dtype=torch.float, device=device)\n",
    "                reward = torch.tensor(reward, dtype=torch.float, device=device)\n",
    "                \n",
    "                # Calculate TD error for PER\n",
    "                with torch.no_grad():\n",
    "                    q_val = policy_dqn(state.unsqueeze(0)).squeeze()[action].item()\n",
    "                    next_q_val = target_dqn(new_state.unsqueeze(0)).max().item()\n",
    "                    td_error = abs(reward.item() + (1 - terminated) * self.discount_factor_g * next_q_val - q_val)\n",
    "\n",
    "\n",
    "                # memory.append((state, action, new_state, reward, terminated))\n",
    "                transition = (state, action, new_state, reward, terminated)\n",
    "                PERmemory.append(transition, priority=td_error)\n",
    "\n",
    "\n",
    "                step_count += 1\n",
    "                state = new_state\n",
    "\n",
    "            rewards_per_episode.append(episode_reward)\n",
    "            \n",
    "\n",
    "            if episode_reward > best_rewards:\n",
    "                torch.save({\n",
    "                    'policy_model_state_dict': policy_dqn.state_dict(),\n",
    "                    'target_model_state_dict': target_dqn.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'epsilon': epsilon,\n",
    "                    'step_count': step_count,\n",
    "                    'best_rewards': best_rewards\n",
    "                }, self.MODEL_FILE)\n",
    "                best_rewards = episode_reward\n",
    "\n",
    "            if episode % 50 == 0:\n",
    "                log_message = f\"Episode {episode}: Total Mean reward = {np.mean(rewards_per_episode)}, Mean Reward last 50 = {np.mean(rewards_per_episode[-50:])}, Epsilon = {epsilon}, best_rewards = {best_rewards}\"\n",
    "                print(log_message)\n",
    "                with open(self.LOG_FILE, 'a') as file:\n",
    "                    file.write(log_message + '\\n')\n",
    "                    \n",
    "            current_time = datetime.now()\n",
    "            if current_time - last_graph_update_time > timedelta(seconds=10):\n",
    "                self.save_graph(rewards_per_episode, epsilon_history)\n",
    "                last_graph_update_time = current_time\n",
    "\n",
    "            if len(PERmemory) > self.mini_batch_size:\n",
    "                transitions, weights, indices = PERmemory.sample(self.mini_batch_size, beta=0.4)\n",
    "                td_errors = self.optimize(transitions, weights, policy_dqn, target_dqn)\n",
    "                PERmemory.update_priorities(indices, abs(td_errors.detach().cpu().numpy()))\n",
    "\n",
    "                \n",
    "            epsilon = max(epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "            epsilon_history.append(epsilon)\n",
    "\n",
    "            if step_count % self.network_sync_rate == 0:\n",
    "                target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "    # Optimize with pytorch\n",
    "    def optimize(self, transitions, weights, policy_dqn, target_dqn):\n",
    "        # Transpose the list of experiences and separate each element\n",
    "        states, actions, new_states, rewards, terminations = zip(*transitions)\n",
    "\n",
    "        # Convert data to tensors for PyTorch to process with GPU\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        new_states = torch.stack(new_states)\n",
    "        rewards = torch.stack(rewards)\n",
    "        terminations = torch.tensor(terminations, dtype=torch.float).to(self.device)\n",
    "        weights = torch.tensor(weights, dtype=torch.float).to(self.device)  # Convert weights to Tensor\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Always use double DQN\n",
    "            best_action_from_policy = policy_dqn(new_states).argmax(dim=1)\n",
    "            target_q = rewards + \\\n",
    "                (1 - terminations) * self.discount_factor_g * \\\n",
    "                target_dqn(new_states).gather(\n",
    "                    dim=1, index=best_action_from_policy.unsqueeze(dim=1)\n",
    "                ).squeeze()\n",
    "\n",
    "        # Calculate Q values from current policy\n",
    "        current_q = policy_dqn(states).gather(dim=1, index=actions.unsqueeze(dim=1)).squeeze()\n",
    "\n",
    "        td_errors = target_q - current_q\n",
    "        loss = (weights * td_errors.pow(2)).mean()\n",
    "        \n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()  # Compute gradients (backpropagation)\n",
    "        self.optimizer.step()\n",
    "        return td_errors\n",
    "\n",
    "    def save_graph(self, rewards_per_episode, epsilon_history):\n",
    "        fig = plt.figure(1)\n",
    "\n",
    "        # Plot average rewards (Y-Axis) vs episodes (X-axis)\n",
    "        mean_rewards = np.zeros(len(rewards_per_episode))\n",
    "        for x in range(len(mean_rewards)):\n",
    "            mean_rewards[x] = np.mean(rewards_per_episode[max(0, x-99):(x+1)])\n",
    "        plt.subplot(121)  # Plot in a 1 row x 2 col grid, at cell 1\n",
    "\n",
    "        plt.ylabel('Mean Rewards')\n",
    "        plt.plot(mean_rewards)\n",
    "\n",
    "        plt.subplot(122)\n",
    "        plt.ylabel('Epsilon Decay')\n",
    "        plt.plot(epsilon_history)\n",
    "\n",
    "        plt.subplots_adjust(wspace=1.0, hspace=1.0)\n",
    "\n",
    "        # save plots\n",
    "        fig.savefig(self.GRAPH_FILE)\n",
    "        plt.close(fig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Start training with hyperparameter\n",
    "\n",
    "below are the hyperparameter settings that works for me.\n",
    "\n",
    "You can Observe the logs in file `runs/Banana_Linux.log`.\n",
    "\n",
    "A PT file is also located in `runs/Banana_linux.pt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1_nodes =  512\n",
      "mini_batch_size =  128\n",
      "Initial trainings\n",
      "12-08 16: 46: 52: Training starting...\n",
      "Episode 0: Total Mean reward = -2.0, Mean Reward last 50 = -2.0, Epsilon = 1, best_rewards = -2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7044/694507319.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action = torch.tensor(action, dtype=torch.int64, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50: Total Mean reward = -0.3137254901960784, Mean Reward last 50 = -0.28, Epsilon = 0.6050060671375365, best_rewards = 2.0\n",
      "Episode 100: Total Mean reward = 0.039603960396039604, Mean Reward last 50 = 0.4, Epsilon = 0.36603234127322926, best_rewards = 4.0\n",
      "Episode 150: Total Mean reward = 0.4105960264900662, Mean Reward last 50 = 1.16, Epsilon = 0.22145178723886094, best_rewards = 6.0\n",
      "Episode 200: Total Mean reward = 0.6865671641791045, Mean Reward last 50 = 1.52, Epsilon = 0.13397967485796175, best_rewards = 9.0\n",
      "Episode 250: Total Mean reward = 0.8645418326693227, Mean Reward last 50 = 1.58, Epsilon = 0.08105851616218133, best_rewards = 12.0\n",
      "Episode 300: Total Mean reward = 1.06312292358804, Mean Reward last 50 = 2.06, Epsilon = 0.04904089407128576, best_rewards = 12.0\n",
      "Episode 350: Total Mean reward = 1.2792022792022792, Mean Reward last 50 = 2.58, Epsilon = 0.029670038450977095, best_rewards = 12.0\n",
      "Episode 400: Total Mean reward = 1.6084788029925188, Mean Reward last 50 = 3.92, Epsilon = 0.017950553275045134, best_rewards = 12.0\n",
      "Episode 450: Total Mean reward = 1.8780487804878048, Mean Reward last 50 = 4.04, Epsilon = 0.010860193639877886, best_rewards = 17.0\n",
      "Episode 500: Total Mean reward = 2.269461077844311, Mean Reward last 50 = 5.8, Epsilon = 0.01, best_rewards = 17.0\n",
      "Episode 550: Total Mean reward = 2.602540834845735, Mean Reward last 50 = 5.94, Epsilon = 0.01, best_rewards = 17.0\n",
      "Episode 600: Total Mean reward = 2.788685524126456, Mean Reward last 50 = 4.84, Epsilon = 0.01, best_rewards = 17.0\n",
      "Episode 650: Total Mean reward = 3.1367127496159752, Mean Reward last 50 = 7.32, Epsilon = 0.01, best_rewards = 17.0\n",
      "Episode 700: Total Mean reward = 3.4593437945791727, Mean Reward last 50 = 7.66, Epsilon = 0.01, best_rewards = 17.0\n",
      "Episode 750: Total Mean reward = 3.696404793608522, Mean Reward last 50 = 7.02, Epsilon = 0.01, best_rewards = 17.0\n",
      "Episode 800: Total Mean reward = 3.9787765293383273, Mean Reward last 50 = 8.22, Epsilon = 0.01, best_rewards = 18.0\n",
      "Episode 850: Total Mean reward = 4.353701527614571, Mean Reward last 50 = 10.36, Epsilon = 0.01, best_rewards = 18.0\n",
      "Episode 900: Total Mean reward = 4.5693673695893455, Mean Reward last 50 = 8.24, Epsilon = 0.01, best_rewards = 18.0\n",
      "Episode 950: Total Mean reward = 4.722397476340694, Mean Reward last 50 = 7.48, Epsilon = 0.01, best_rewards = 18.0\n",
      "Episode 1000: Total Mean reward = 5.016983016983017, Mean Reward last 50 = 10.62, Epsilon = 0.01, best_rewards = 18.0\n",
      "Episode 1050: Total Mean reward = 5.320647002854424, Mean Reward last 50 = 11.4, Epsilon = 0.01, best_rewards = 18.0\n",
      "Episode 1100: Total Mean reward = 5.569482288828338, Mean Reward last 50 = 10.8, Epsilon = 0.01, best_rewards = 18.0\n",
      "Episode 1150: Total Mean reward = 5.756733275412684, Mean Reward last 50 = 9.88, Epsilon = 0.01, best_rewards = 18.0\n",
      "Episode 1200: Total Mean reward = 5.872606161532056, Mean Reward last 50 = 8.54, Epsilon = 0.01, best_rewards = 19.0\n",
      "Episode 1250: Total Mean reward = 6.085531574740208, Mean Reward last 50 = 11.2, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 1300: Total Mean reward = 6.156802459646426, Mean Reward last 50 = 7.94, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 1350: Total Mean reward = 6.223538119911177, Mean Reward last 50 = 7.96, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 1400: Total Mean reward = 6.337615988579586, Mean Reward last 50 = 9.42, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 1450: Total Mean reward = 6.32184700206754, Mean Reward last 50 = 5.88, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 1500: Total Mean reward = 6.447035309793471, Mean Reward last 50 = 10.08, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 1550: Total Mean reward = 6.527401676337846, Mean Reward last 50 = 8.94, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 1600: Total Mean reward = 6.705184259837601, Mean Reward last 50 = 12.22, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 1650: Total Mean reward = 6.774076317383404, Mean Reward last 50 = 8.98, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 1700: Total Mean reward = 6.878894767783657, Mean Reward last 50 = 10.34, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 1750: Total Mean reward = 6.975442604226156, Mean Reward last 50 = 10.26, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 1800: Total Mean reward = 7.113270405330372, Mean Reward last 50 = 11.94, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 1850: Total Mean reward = 7.173419773095624, Mean Reward last 50 = 9.34, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 1900: Total Mean reward = 7.234087322461862, Mean Reward last 50 = 9.48, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 1950: Total Mean reward = 7.297283444387493, Mean Reward last 50 = 9.7, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 2000: Total Mean reward = 7.318840579710145, Mean Reward last 50 = 8.16, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 2050: Total Mean reward = 7.3973671379814725, Mean Reward last 50 = 10.54, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 2100: Total Mean reward = 7.409804854831033, Mean Reward last 50 = 7.92, Epsilon = 0.01, best_rewards = 21.0\n",
      "Episode 2150: Total Mean reward = 7.549976754997675, Mean Reward last 50 = 13.44, Epsilon = 0.01, best_rewards = 22.0\n",
      "Episode 2200: Total Mean reward = 7.644252612448887, Mean Reward last 50 = 11.7, Epsilon = 0.01, best_rewards = 22.0\n",
      "Episode 2250: Total Mean reward = 7.651710350955131, Mean Reward last 50 = 7.98, Epsilon = 0.01, best_rewards = 22.0\n",
      "Episode 2300: Total Mean reward = 7.720990873533246, Mean Reward last 50 = 10.84, Epsilon = 0.01, best_rewards = 22.0\n",
      "Episode 2350: Total Mean reward = 7.789451297320289, Mean Reward last 50 = 10.94, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 2400: Total Mean reward = 7.885047896709704, Mean Reward last 50 = 12.38, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 2450: Total Mean reward = 7.981232150142799, Mean Reward last 50 = 12.6, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 2500: Total Mean reward = 8.00919632147141, Mean Reward last 50 = 9.38, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 2550: Total Mean reward = 8.075264602116818, Mean Reward last 50 = 11.38, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 2600: Total Mean reward = 8.125720876585929, Mean Reward last 50 = 10.7, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 2650: Total Mean reward = 8.225575254620898, Mean Reward last 50 = 13.42, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 2700: Total Mean reward = 8.27360236949278, Mean Reward last 50 = 10.82, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 2750: Total Mean reward = 8.342420937840785, Mean Reward last 50 = 12.06, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 2800: Total Mean reward = 8.402356301320957, Mean Reward last 50 = 11.7, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 2850: Total Mean reward = 8.445457734128375, Mean Reward last 50 = 10.86, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 2900: Total Mean reward = 8.437435367114787, Mean Reward last 50 = 7.98, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 2950: Total Mean reward = 8.488309047780414, Mean Reward last 50 = 11.44, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3000: Total Mean reward = 8.47884038653782, Mean Reward last 50 = 7.92, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3050: Total Mean reward = 8.535889872173058, Mean Reward last 50 = 11.96, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3100: Total Mean reward = 8.589164785553047, Mean Reward last 50 = 11.84, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3150: Total Mean reward = 8.639479530307838, Mean Reward last 50 = 11.76, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3200: Total Mean reward = 8.710715401437051, Mean Reward last 50 = 13.2, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3250: Total Mean reward = 8.763457397723778, Mean Reward last 50 = 12.14, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3300: Total Mean reward = 8.80036352620418, Mean Reward last 50 = 11.2, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3350: Total Mean reward = 8.810205908683974, Mean Reward last 50 = 9.46, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3400: Total Mean reward = 8.834166421640694, Mean Reward last 50 = 10.44, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3450: Total Mean reward = 8.847000869313243, Mean Reward last 50 = 9.72, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3500: Total Mean reward = 8.919165952584976, Mean Reward last 50 = 13.9, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3550: Total Mean reward = 8.958039988735568, Mean Reward last 50 = 11.68, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3600: Total Mean reward = 8.990835878922521, Mean Reward last 50 = 11.32, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3650: Total Mean reward = 9.016981648863325, Mean Reward last 50 = 10.9, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3700: Total Mean reward = 9.065117535801134, Mean Reward last 50 = 12.58, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3750: Total Mean reward = 9.119168221807518, Mean Reward last 50 = 13.12, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3800: Total Mean reward = 9.158379373848987, Mean Reward last 50 = 12.1, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3850: Total Mean reward = 9.229291093222539, Mean Reward last 50 = 14.62, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3900: Total Mean reward = 9.288387592924892, Mean Reward last 50 = 13.84, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 3950: Total Mean reward = 9.358643381422425, Mean Reward last 50 = 14.84, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4000: Total Mean reward = 9.414896275931017, Mean Reward last 50 = 13.86, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4050: Total Mean reward = 9.48012836336707, Mean Reward last 50 = 14.7, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4100: Total Mean reward = 9.533284564740308, Mean Reward last 50 = 13.84, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4150: Total Mean reward = 9.525415562515057, Mean Reward last 50 = 8.88, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4200: Total Mean reward = 9.566769816710307, Mean Reward last 50 = 13.0, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4250: Total Mean reward = 9.603152199482475, Mean Reward last 50 = 12.66, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4300: Total Mean reward = 9.63496861195071, Mean Reward last 50 = 12.34, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4350: Total Mean reward = 9.680533210756147, Mean Reward last 50 = 13.6, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4400: Total Mean reward = 9.713019768234492, Mean Reward last 50 = 12.54, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4450: Total Mean reward = 9.756009885419006, Mean Reward last 50 = 13.54, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4500: Total Mean reward = 9.750277716063097, Mean Reward last 50 = 9.24, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4550: Total Mean reward = 9.778949681388706, Mean Reward last 50 = 12.36, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4600: Total Mean reward = 9.839600086937622, Mean Reward last 50 = 15.36, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4650: Total Mean reward = 9.878950763276714, Mean Reward last 50 = 13.5, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4700: Total Mean reward = 9.914699000212721, Mean Reward last 50 = 13.24, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4750: Total Mean reward = 9.960429383287728, Mean Reward last 50 = 14.26, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4800: Total Mean reward = 9.995834201208082, Mean Reward last 50 = 13.36, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4850: Total Mean reward = 10.024531024531024, Mean Reward last 50 = 12.78, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4900: Total Mean reward = 10.070393797184249, Mean Reward last 50 = 14.52, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 4950: Total Mean reward = 10.109068874974753, Mean Reward last 50 = 13.9, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 5000: Total Mean reward = 10.15876824635073, Mean Reward last 50 = 15.08, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 5050: Total Mean reward = 10.217778657691547, Mean Reward last 50 = 16.12, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 5100: Total Mean reward = 10.261517349539305, Mean Reward last 50 = 14.68, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 5150: Total Mean reward = 10.296641428848767, Mean Reward last 50 = 13.88, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 5200: Total Mean reward = 10.34262641799654, Mean Reward last 50 = 15.08, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 5250: Total Mean reward = 10.372690916015998, Mean Reward last 50 = 13.5, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 5300: Total Mean reward = 10.407470288624788, Mean Reward last 50 = 14.06, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 5350: Total Mean reward = 10.445150439170249, Mean Reward last 50 = 14.44, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 5400: Total Mean reward = 10.485835956304388, Mean Reward last 50 = 14.84, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 5450: Total Mean reward = 10.526141992294992, Mean Reward last 50 = 14.88, Epsilon = 0.01, best_rewards = 24.0\n",
      "Episode 5500: Total Mean reward = 10.578440283584802, Mean Reward last 50 = 16.28, Epsilon = 0.01, best_rewards = 25.0\n",
      "Episode 5550: Total Mean reward = 10.631417762565304, Mean Reward last 50 = 16.46, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 5600: Total Mean reward = 10.679700053561865, Mean Reward last 50 = 16.04, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 5650: Total Mean reward = 10.704477083701999, Mean Reward last 50 = 13.48, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 5700: Total Mean reward = 10.733730924399229, Mean Reward last 50 = 14.04, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 5750: Total Mean reward = 10.765779864371414, Mean Reward last 50 = 14.42, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 5800: Total Mean reward = 10.792621961730736, Mean Reward last 50 = 13.88, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 5850: Total Mean reward = 10.83216544180482, Mean Reward last 50 = 15.42, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 5900: Total Mean reward = 10.850533807829182, Mean Reward last 50 = 13.0, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 5950: Total Mean reward = 10.869601747605444, Mean Reward last 50 = 13.12, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6000: Total Mean reward = 10.886352274620897, Mean Reward last 50 = 12.88, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6050: Total Mean reward = 10.908940670963478, Mean Reward last 50 = 13.62, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6100: Total Mean reward = 10.945090968693657, Mean Reward last 50 = 15.32, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6150: Total Mean reward = 10.954478946512761, Mean Reward last 50 = 12.1, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6200: Total Mean reward = 10.97597161748105, Mean Reward last 50 = 13.62, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6250: Total Mean reward = 11.015677491601343, Mean Reward last 50 = 15.94, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6300: Total Mean reward = 11.033804158070147, Mean Reward last 50 = 13.3, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6350: Total Mean reward = 11.046921744607149, Mean Reward last 50 = 12.7, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6400: Total Mean reward = 11.084205592876113, Mean Reward last 50 = 15.82, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6450: Total Mean reward = 11.110370485196094, Mean Reward last 50 = 14.46, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6500: Total Mean reward = 11.11767420396862, Mean Reward last 50 = 12.06, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6550: Total Mean reward = 11.148679590902152, Mean Reward last 50 = 15.18, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6600: Total Mean reward = 11.173458566883806, Mean Reward last 50 = 14.42, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6650: Total Mean reward = 11.195459329424146, Mean Reward last 50 = 14.1, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6700: Total Mean reward = 11.231010296970602, Mean Reward last 50 = 15.96, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6750: Total Mean reward = 11.254628943860169, Mean Reward last 50 = 14.42, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6800: Total Mean reward = 11.272606969563299, Mean Reward last 50 = 13.7, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6850: Total Mean reward = 11.294555539337322, Mean Reward last 50 = 14.28, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6900: Total Mean reward = 11.306477322127227, Mean Reward last 50 = 12.94, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 6950: Total Mean reward = 11.332470148180118, Mean Reward last 50 = 14.92, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7000: Total Mean reward = 11.364519354377945, Mean Reward last 50 = 15.82, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7050: Total Mean reward = 11.387179123528577, Mean Reward last 50 = 14.56, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7100: Total Mean reward = 11.416983523447401, Mean Reward last 50 = 15.62, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7150: Total Mean reward = 11.448049223884771, Mean Reward last 50 = 15.86, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7200: Total Mean reward = 11.47479516733787, Mean Reward last 50 = 15.3, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7250: Total Mean reward = 11.506826644600745, Mean Reward last 50 = 16.12, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7300: Total Mean reward = 11.539104232296946, Mean Reward last 50 = 16.22, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7350: Total Mean reward = 11.565093184600734, Mean Reward last 50 = 15.36, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7400: Total Mean reward = 11.59248750168896, Mean Reward last 50 = 15.62, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7450: Total Mean reward = 11.610253657227219, Mean Reward last 50 = 14.24, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7500: Total Mean reward = 11.63498200239968, Mean Reward last 50 = 15.32, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7550: Total Mean reward = 11.65421798437293, Mean Reward last 50 = 14.54, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7600: Total Mean reward = 11.679647414813841, Mean Reward last 50 = 15.52, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7650: Total Mean reward = 11.70448307410796, Mean Reward last 50 = 15.48, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7700: Total Mean reward = 11.728606674457863, Mean Reward last 50 = 15.42, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7750: Total Mean reward = 11.755644432976391, Mean Reward last 50 = 15.92, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7800: Total Mean reward = 11.780156390206384, Mean Reward last 50 = 15.58, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7850: Total Mean reward = 11.794293720545154, Mean Reward last 50 = 14.0, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7900: Total Mean reward = 11.814706999114037, Mean Reward last 50 = 15.02, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 7950: Total Mean reward = 11.8385108791347, Mean Reward last 50 = 15.6, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8000: Total Mean reward = 11.866266716660418, Mean Reward last 50 = 16.28, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8050: Total Mean reward = 11.890075766985468, Mean Reward last 50 = 15.7, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8100: Total Mean reward = 11.921244290828293, Mean Reward last 50 = 16.94, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8150: Total Mean reward = 11.94675499938658, Mean Reward last 50 = 16.08, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8200: Total Mean reward = 11.964882331422997, Mean Reward last 50 = 14.92, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8250: Total Mean reward = 11.974063749848503, Mean Reward last 50 = 13.48, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8300: Total Mean reward = 11.99638597759306, Mean Reward last 50 = 15.68, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8350: Total Mean reward = 12.018440905280805, Mean Reward last 50 = 15.68, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8400: Total Mean reward = 12.041899773836448, Mean Reward last 50 = 15.96, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8450: Total Mean reward = 12.05845462075494, Mean Reward last 50 = 14.84, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8500: Total Mean reward = 12.0731678626044, Mean Reward last 50 = 14.56, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8550: Total Mean reward = 12.09636299847971, Mean Reward last 50 = 16.04, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8600: Total Mean reward = 12.11545169166376, Mean Reward last 50 = 15.38, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8650: Total Mean reward = 12.138018726158826, Mean Reward last 50 = 16.02, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8700: Total Mean reward = 12.159177106079762, Mean Reward last 50 = 15.82, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8750: Total Mean reward = 12.172780253685293, Mean Reward last 50 = 14.54, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8800: Total Mean reward = 12.197704806272014, Mean Reward last 50 = 16.56, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8850: Total Mean reward = 12.222121794147554, Mean Reward last 50 = 16.52, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8900: Total Mean reward = 12.249410178631614, Mean Reward last 50 = 17.08, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 8950: Total Mean reward = 12.272148363311361, Mean Reward last 50 = 16.32, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9000: Total Mean reward = 12.285968225752693, Mean Reward last 50 = 14.76, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9050: Total Mean reward = 12.299303944315545, Mean Reward last 50 = 14.7, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9100: Total Mean reward = 12.32172288759477, Mean Reward last 50 = 16.38, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9150: Total Mean reward = 12.33941645721779, Mean Reward last 50 = 15.56, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9200: Total Mean reward = 12.354200630366265, Mean Reward last 50 = 15.06, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9250: Total Mean reward = 12.38049940546968, Mean Reward last 50 = 17.22, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9300: Total Mean reward = 12.398236748736695, Mean Reward last 50 = 15.68, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9350: Total Mean reward = 12.415249705913807, Mean Reward last 50 = 15.58, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9400: Total Mean reward = 12.434847356664184, Mean Reward last 50 = 16.1, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9450: Total Mean reward = 12.455084118082743, Mean Reward last 50 = 16.26, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9500: Total Mean reward = 12.472160825176298, Mean Reward last 50 = 15.7, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9550: Total Mean reward = 12.489058737304994, Mean Reward last 50 = 15.7, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9600: Total Mean reward = 12.505988959483387, Mean Reward last 50 = 15.74, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9650: Total Mean reward = 12.523676302973785, Mean Reward last 50 = 15.92, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9700: Total Mean reward = 12.5385011854448, Mean Reward last 50 = 15.4, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9750: Total Mean reward = 12.548354014972823, Mean Reward last 50 = 14.46, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9800: Total Mean reward = 12.56177941026426, Mean Reward last 50 = 15.18, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9850: Total Mean reward = 12.58034717287585, Mean Reward last 50 = 16.22, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9900: Total Mean reward = 12.598828401171598, Mean Reward last 50 = 16.24, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 9950: Total Mean reward = 12.607476635514018, Mean Reward last 50 = 14.32, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10000: Total Mean reward = 12.61003899610039, Mean Reward last 50 = 13.12, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10050: Total Mean reward = 12.584319968162372, Mean Reward last 50 = 7.44, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10100: Total Mean reward = 12.559251559251559, Mean Reward last 50 = 7.52, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10150: Total Mean reward = 12.582307161855974, Mean Reward last 50 = 17.24, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10200: Total Mean reward = 12.593667287520832, Mean Reward last 50 = 14.9, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10250: Total Mean reward = 12.607550482879718, Mean Reward last 50 = 15.44, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10300: Total Mean reward = 12.629550529074848, Mean Reward last 50 = 17.14, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10350: Total Mean reward = 12.6427398319003, Mean Reward last 50 = 15.36, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10400: Total Mean reward = 12.653687145466781, Mean Reward last 50 = 14.92, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10450: Total Mean reward = 12.671419002966223, Mean Reward last 50 = 16.36, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10500: Total Mean reward = 12.686315588991524, Mean Reward last 50 = 15.8, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10550: Total Mean reward = 12.702397876978486, Mean Reward last 50 = 16.08, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10600: Total Mean reward = 12.717196490897086, Mean Reward last 50 = 15.84, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10650: Total Mean reward = 12.736738334428692, Mean Reward last 50 = 16.88, Epsilon = 0.01, best_rewards = 26.0\n",
      "Episode 10700: Total Mean reward = 12.750864405195776, Mean Reward last 50 = 15.76, Epsilon = 0.01, best_rewards = 26.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# for printing date and time\n",
    "DATE_FORMAT = \"%m-%d %H: %M: %S\"\n",
    "\n",
    "# Directory for saving run info\n",
    "RUNS_DIR = \"runs\"\n",
    "os.makedirs(RUNS_DIR, exist_ok=True)\n",
    "\n",
    "# 'Agg': used to generate plots as images and save them to a file instead of\n",
    "# rendering to screen\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "env.reset()\n",
    "trainer = DqnTrainer('Banana_Linux', brain_name, RUNS_DIR, DATE_FORMAT)\n",
    "trainer.learning_rate_a =  0.0005 \n",
    "trainer.discount_factor_g = 0.99\n",
    "trainer.network_sync_rate = 500\n",
    "trainer.replay_memory_size = 1_000_000\n",
    "trainer.mini_batch_size = 128\n",
    "trainer.epsilon_init = 1\n",
    "trainer.epsilon_decay = 0.99\n",
    "trainer.epsilon_min = 0.01\n",
    "trainer.stop_on_reward = 15\n",
    "trainer.fc1_nodes = 512  \n",
    "\n",
    "trainer.train(env, env_info, state_size, action_size, device, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
